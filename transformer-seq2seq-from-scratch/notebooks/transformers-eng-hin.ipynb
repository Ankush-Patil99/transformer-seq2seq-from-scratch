{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":1355361,"sourceType":"datasetVersion","datasetId":789090,"isSourceIdPinned":false}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Environment setup","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\nimport pandas as pd\nimport random\nimport matplotlib.pyplot as plt\n\nimport sacrebleu\n\nprint(\"PyTorch version:\", torch.__version__)\nprint(\"GPU available:\", torch.cuda.is_available())\nif torch.cuda.is_available():\n    print(\"GPU name:\", torch.cuda.get_device_name(0))\n\nSEED = 42\ntorch.manual_seed(SEED)\nnp.random.seed(SEED)\nrandom.seed(SEED)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(SEED)\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-29T09:08:28.708744Z","iopub.execute_input":"2025-11-29T09:08:28.709239Z","iopub.status.idle":"2025-11-29T09:08:32.061586Z","shell.execute_reply.started":"2025-11-29T09:08:28.709216Z","shell.execute_reply":"2025-11-29T09:08:32.060593Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Dataset loading","metadata":{}},{"cell_type":"code","source":"import kagglehub\n\n# Download latest version\npath = kagglehub.dataset_download(\"vaibhavkumar11/hindi-english-parallel-corpus\")\n\nprint(\"Path to dataset files:\", path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T09:19:31.271041Z","iopub.execute_input":"2025-11-29T09:19:31.271826Z","iopub.status.idle":"2025-11-29T09:19:49.490347Z","shell.execute_reply.started":"2025-11-29T09:19:31.271797Z","shell.execute_reply":"2025-11-29T09:19:49.489583Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"path = \"/kaggle/input/hindi-english-parallel-corpus/hindi_english_parallel.csv\"\ndf = pd.read_csv(path)\n\nprint(df.columns)\ndf.head()\ndf.describe()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T09:26:22.036207Z","iopub.execute_input":"2025-11-29T09:26:22.036832Z","iopub.status.idle":"2025-11-29T09:26:30.472273Z","shell.execute_reply.started":"2025-11-29T09:26:22.036809Z","shell.execute_reply":"2025-11-29T09:26:30.471695Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df = df[['english', 'hindi']].dropna()\n\ndf = df.rename(columns={'english': 'en', 'hindi': 'hi'})\n\ndf['en'] = df['en'].str.strip().str.lower()\ndf['hi'] = df['hi'].str.strip().str.lower()\n\n# Remove extremely long sentences (RAM safety)\ndf = df[df['en'].str.len() < 200]\ndf = df[df['hi'].str.len() < 200]\n\n# Keep only 10,000 examples for smooth training\ndf = df.head(10000).reset_index(drop=True)\n\nprint(df.head())\nprint(\"Total usable pairs:\", len(df))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T09:29:33.172674Z","iopub.execute_input":"2025-11-29T09:29:33.173457Z","iopub.status.idle":"2025-11-29T09:29:37.427558Z","shell.execute_reply.started":"2025-11-29T09:29:33.173431Z","shell.execute_reply":"2025-11-29T09:29:37.426770Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Train-test split","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\ntrain_df, val_df = train_test_split(\n    df[['en', 'hi']],\n    test_size=0.2,\n    random_state=42\n)\n\ntrain_df = train_df.reset_index(drop=True)\nval_df = val_df.reset_index(drop=True)\n\nprint(\"Train size:\", len(train_df))\nprint(\"Validation size:\", len(val_df))\ntrain_df.head()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T09:30:46.721138Z","iopub.execute_input":"2025-11-29T09:30:46.721752Z","iopub.status.idle":"2025-11-29T09:30:47.636222Z","shell.execute_reply.started":"2025-11-29T09:30:46.721725Z","shell.execute_reply":"2025-11-29T09:30:47.635537Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Tokenizer,vocabulary,encoder and dataloaders","metadata":{}},{"cell_type":"code","source":"from collections import Counter\n\ndef tokenize(text):\n    return text.split()\n\n# Build vocabulary\ndef build_vocab(sentences, max_size=8000):\n    counter = Counter()\n    for s in sentences:\n        counter.update(tokenize(s))\n\n    # Reserve 4 special tokens\n    vocab = {\"<pad>\": 0, \"<sos>\": 1, \"<eos>\": 2, \"<unk>\": 3}\n\n    most_common = counter.most_common(max_size - 4)\n    for i, (tok, _) in enumerate(most_common, start=4):\n        vocab[tok] = i\n\n    return vocab\n\n# Build both vocabs\nvocab_en = build_vocab(train_df['en'])\nvocab_hi = build_vocab(train_df['hi'])\n\nprint(\"English vocab size:\", len(vocab_en))\nprint(\"Hindi vocab size:\", len(vocab_hi))\n\n# Encoding function\ndef encode(sentence, vocab):\n    tokens = tokenize(sentence)\n    ids = [vocab.get(t, 3) for t in tokens]  # 3 = <unk>\n    return [1] + ids + [2]  # <sos> ... <eos>\n\n# Padding\ndef pad_batch(seqs, pad_id=0):\n    max_len = max(len(s) for s in seqs)\n    return torch.tensor([s + [pad_id] * (max_len - len(s)) for s in seqs])\n\n# Dataset class\nclass TranslationDataset(torch.utils.data.Dataset):\n    def __init__(self, df, vocab_en, vocab_hi):\n        self.en = df['en'].tolist()\n        self.hi = df['hi'].tolist()\n        self.vocab_en = vocab_en\n        self.vocab_hi = vocab_hi\n\n    def __len__(self):\n        return len(self.en)\n\n    def __getitem__(self, idx):\n        en_ids = encode(self.en[idx], self.vocab_en)\n        hi_ids = encode(self.hi[idx], self.vocab_hi)\n        return en_ids, hi_ids\n\n# Collate for DataLoader\ndef collate_fn(batch):\n    en_batch = [b[0] for b in batch]\n    hi_batch = [b[1] for b in batch]\n\n    en_pad = pad_batch(en_batch, pad_id=0)\n    hi_pad = pad_batch(hi_batch, pad_id=0)\n\n    return en_pad, hi_pad\n\n# DataLoaders\ntrain_loader = torch.utils.data.DataLoader(\n    TranslationDataset(train_df, vocab_en, vocab_hi),\n    batch_size=32,\n    shuffle=True,\n    collate_fn=collate_fn\n)\n\nval_loader = torch.utils.data.DataLoader(\n    TranslationDataset(val_df, vocab_en, vocab_hi),\n    batch_size=32,\n    shuffle=False,\n    collate_fn=collate_fn\n)\n\n# Test batch\nfor en, hi in train_loader:\n    print(\"English batch shape:\", en.shape)\n    print(\"Hindi batch shape:\", hi.shape)\n    break\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T09:31:59.474577Z","iopub.execute_input":"2025-11-29T09:31:59.475491Z","iopub.status.idle":"2025-11-29T09:31:59.562648Z","shell.execute_reply.started":"2025-11-29T09:31:59.475467Z","shell.execute_reply":"2025-11-29T09:31:59.561875Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Positional Embeddings(Sinusoidal or learned)","metadata":{}},{"cell_type":"code","source":"import math\n\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model, max_len=200):\n        super().__init__()\n        \n        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * \n                             (-math.log(10000.0) / d_model))\n\n        pe[:, 0::2] = torch.sin(position * div_term)   # even indices\n        pe[:, 1::2] = torch.cos(position * div_term)   # odd indices\n        \n        pe = pe.unsqueeze(0)   # shape: (1, max_len, d_model)\n        self.register_buffer('pe', pe)\n\n    def forward(self, x):\n        # x: (batch, seq_len, d_model)\n        seq_len = x.size(1)\n        return x + self.pe[:, :seq_len]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T09:33:41.010262Z","iopub.execute_input":"2025-11-29T09:33:41.010992Z","iopub.status.idle":"2025-11-29T09:33:41.016438Z","shell.execute_reply.started":"2025-11-29T09:33:41.010967Z","shell.execute_reply":"2025-11-29T09:33:41.015880Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Scaled Dot-Product attention","metadata":{}},{"cell_type":"code","source":"\nclass ScaledDotProductAttention(nn.Module):\n    def __init__(self, d_k):\n        super().__init__()\n        self.d_k = d_k\n\n    def forward(self, Q, K, V, mask=None):\n        # Q, K, V: (batch, heads, seq_len, d_k)\n        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n\n        if mask is not None:\n            scores = scores.masked_fill(mask == 0, -1e9)\n\n        attn = torch.softmax(scores, dim=-1)\n        output = torch.matmul(attn, V)\n\n        return output, attn\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T09:34:40.466429Z","iopub.execute_input":"2025-11-29T09:34:40.467023Z","iopub.status.idle":"2025-11-29T09:34:40.472121Z","shell.execute_reply.started":"2025-11-29T09:34:40.466998Z","shell.execute_reply":"2025-11-29T09:34:40.471372Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Multihead attention ","metadata":{}},{"cell_type":"code","source":"!pip install einops sacrebleu --quiet\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T09:36:04.691646Z","iopub.execute_input":"2025-11-29T09:36:04.692289Z","iopub.status.idle":"2025-11-29T09:36:07.899080Z","shell.execute_reply.started":"2025-11-29T09:36:04.692267Z","shell.execute_reply":"2025-11-29T09:36:07.898060Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class MultiHeadAttention(nn.Module):\n    def __init__(self, d_model, num_heads):\n        super().__init__()\n        assert d_model % num_heads == 0\n        \n        self.num_heads = num_heads\n        self.d_k = d_model // num_heads\n        \n        self.W_q = nn.Linear(d_model, d_model)\n        self.W_k = nn.Linear(d_model, d_model)\n        self.W_v = nn.Linear(d_model, d_model)\n        \n        self.attention = ScaledDotProductAttention(self.d_k)\n        self.W_o = nn.Linear(d_model, d_model)\n\n    def forward(self, x_q, x_k, x_v, mask=None):\n        if mask is not None:\n            # FIX: remove redundant dimension\n            if mask.dim() == 4:\n                mask = mask.squeeze(1)  # (B,1,L,L)->(B,L,L)\n\n            # Expand mask for heads\n            mask = mask.unsqueeze(1)   # (B,1,L,L)->(B,H,L,L)\n\n        B, L, _ = x_q.size()\n\n        # Linear projections\n        Q = self.W_q(x_q)\n        K = self.W_k(x_k)\n        V = self.W_v(x_v)\n\n        # Split into heads\n        Q = rearrange(Q, \"b l (h d) -> b h l d\", h=self.num_heads)\n        K = rearrange(K, \"b l (h d) -> b h l d\", h=self.num_heads)\n        V = rearrange(V, \"b l (h d) -> b h l d\", h=self.num_heads)\n\n        # Apply attention\n        context, attn = self.attention(Q, K, V, mask)\n\n        # Merge heads\n        context = rearrange(context, \"b h l d -> b l (h d)\")\n        output = self.W_o(context)\n\n        return output, attn\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T10:36:56.250308Z","iopub.execute_input":"2025-11-29T10:36:56.250890Z","iopub.status.idle":"2025-11-29T10:36:56.257816Z","shell.execute_reply.started":"2025-11-29T10:36:56.250864Z","shell.execute_reply":"2025-11-29T10:36:56.257076Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Fee forward Neural net","metadata":{}},{"cell_type":"code","source":"class FeedForwardNetwork(nn.Module):\n    def __init__(self, d_model, d_ff):\n        super().__init__()\n        self.fc1 = nn.Linear(d_model, d_ff)\n        self.fc2 = nn.Linear(d_ff, d_model)\n\n    def forward(self, x):\n        return self.fc2(F.relu(self.fc1(x)))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T10:09:23.736925Z","iopub.execute_input":"2025-11-29T10:09:23.737448Z","iopub.status.idle":"2025-11-29T10:09:23.741490Z","shell.execute_reply.started":"2025-11-29T10:09:23.737428Z","shell.execute_reply":"2025-11-29T10:09:23.740831Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Layer norm and residual connections","metadata":{}},{"cell_type":"code","source":"class ResidualConnection(nn.Module):\n    def __init__(self, d_model, dropout=0.1):\n        super().__init__()\n        self.norm = nn.LayerNorm(d_model)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x, sublayer_output):\n        return self.norm(x + self.dropout(sublayer_output))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T10:09:29.133010Z","iopub.execute_input":"2025-11-29T10:09:29.133263Z","iopub.status.idle":"2025-11-29T10:09:29.137476Z","shell.execute_reply.started":"2025-11-29T10:09:29.133247Z","shell.execute_reply":"2025-11-29T10:09:29.136594Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Encoder Block","metadata":{}},{"cell_type":"code","source":"# Self attention + FFN\nclass EncoderBlock(nn.Module):\n    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n        super().__init__()\n        self.self_attn = MultiHeadAttention(d_model, num_heads)\n        self.res1 = ResidualConnection(d_model, dropout)\n        \n        self.ffn = FeedForwardNetwork(d_model, d_ff)\n        self.res2 = ResidualConnection(d_model, dropout)\n\n    def forward(self, x, mask):\n        # 1) Self-attention\n        attn_output, _ = self.self_attn(x, x, x, mask)\n        x = self.res1(x, attn_output)\n\n        # 2) Feed-forward network\n        ffn_output = self.ffn(x)\n        x = self.res2(x, ffn_output)\n\n        return x\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T10:37:04.620011Z","iopub.execute_input":"2025-11-29T10:37:04.620540Z","iopub.status.idle":"2025-11-29T10:37:04.625574Z","shell.execute_reply.started":"2025-11-29T10:37:04.620515Z","shell.execute_reply":"2025-11-29T10:37:04.624747Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Stacked Encoder(N layers)","metadata":{}},{"cell_type":"code","source":"class Encoder(nn.Module):\n    def __init__(self, vocab_size, d_model, num_heads, d_ff, num_layers, dropout=0.1, max_len=200):\n        super().__init__()\n\n        # Token embeddings\n        self.embedding = nn.Embedding(vocab_size, d_model)\n\n        # Positional encodings\n        self.pos_encoding = PositionalEncoding(d_model, max_len)\n\n        # Stack encoder blocks\n        self.layers = nn.ModuleList([\n            EncoderBlock(d_model, num_heads, d_ff, dropout)\n            for _ in range(num_layers)\n        ])\n\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x, mask):\n        # x: (batch, seq_len)\n\n        x = self.embedding(x)        # (B, L, D)\n        x = self.pos_encoding(x)     # add positional encodings\n        x = self.dropout(x)\n\n        for layer in self.layers:\n            x = layer(x, mask)\n\n        return x    \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T10:37:09.849341Z","iopub.execute_input":"2025-11-29T10:37:09.849649Z","iopub.status.idle":"2025-11-29T10:37:09.855168Z","shell.execute_reply.started":"2025-11-29T10:37:09.849627Z","shell.execute_reply":"2025-11-29T10:37:09.854410Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Decoder block","metadata":{}},{"cell_type":"code","source":"# Masked self attention + Cross attention +  FF NN\n\nclass DecoderBlock(nn.Module):\n    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n        super().__init__()\n\n        # 1) Masked self-attention\n        self.self_attn = MultiHeadAttention(d_model, num_heads)\n        self.res1 = ResidualConnection(d_model, dropout)\n\n        # 2) Cross-attention (encoder → decoder)\n        self.cross_attn = MultiHeadAttention(d_model, num_heads)\n        self.res2 = ResidualConnection(d_model, dropout)\n\n        # 3) Feed-forward network\n        self.ffn = FeedForwardNetwork(d_model, d_ff)\n        self.res3 = ResidualConnection(d_model, dropout)\n\n    def forward(self, x, enc_out, trg_mask, src_mask):\n        # x: decoder input\n        # enc_out: encoder output\n\n        # 1) Masked self-attention\n        self_attn_out, _ = self.self_attn(x, x, x, trg_mask)\n        x = self.res1(x, self_attn_out)\n\n        # 2) Cross-attention (decoder queries, encoder memory as key/value)\n        cross_attn_out, attn_weights = self.cross_attn(x, enc_out, enc_out, src_mask)\n        x = self.res2(x, cross_attn_out)\n\n        # 3) FFN\n        ffn_out = self.ffn(x)\n        x = self.res3(x, ffn_out)\n\n        return x, attn_weights  \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T10:37:15.457974Z","iopub.execute_input":"2025-11-29T10:37:15.458618Z","iopub.status.idle":"2025-11-29T10:37:15.464241Z","shell.execute_reply.started":"2025-11-29T10:37:15.458594Z","shell.execute_reply":"2025-11-29T10:37:15.463396Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Stacked Decoder","metadata":{}},{"cell_type":"code","source":"class Decoder(nn.Module):\n    def __init__(self, vocab_size, d_model, num_heads, d_ff, num_layers, dropout=0.1, max_len=200):\n        super().__init__()\n\n        self.embedding = nn.Embedding(vocab_size, d_model)\n        self.pos_encoding = PositionalEncoding(d_model, max_len)\n        self.dropout = nn.Dropout(dropout)\n\n        self.layers = nn.ModuleList([\n            DecoderBlock(d_model, num_heads, d_ff, dropout)\n            for _ in range(num_layers)\n        ])\n\n        self.final_norm = nn.LayerNorm(d_model)\n\n    def forward(self, x, enc_out, trg_mask, src_mask):\n        # x: decoder input \n        # enc_out: encoder output \n\n        x = self.embedding(x)\n        x = self.pos_encoding(x)\n        x = self.dropout(x)\n\n        attn_maps = []   # storinf cross attention\n\n        for layer in self.layers:\n            x, attn_weights = layer(x, enc_out, trg_mask, src_mask)\n            attn_maps.append(attn_weights)\n\n        x = self.final_norm(x)\n        return x, attn_maps\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T10:37:17.887451Z","iopub.execute_input":"2025-11-29T10:37:17.888081Z","iopub.status.idle":"2025-11-29T10:37:17.894080Z","shell.execute_reply.started":"2025-11-29T10:37:17.888054Z","shell.execute_reply":"2025-11-29T10:37:17.893371Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Complete Transformer Model","metadata":{}},{"cell_type":"code","source":"class Transformer(nn.Module):\n    def __init__(\n        self,\n        src_vocab_size,   # English vocab\n        trg_vocab_size,   # Hindi vocab\n        d_model=256,\n        num_heads=4,\n        num_layers=3,\n        d_ff=512,\n        dropout=0.1,\n        max_len=200\n    ):\n        super().__init__()\n\n        # Encoder\n        self.encoder = Encoder(\n            vocab_size=src_vocab_size,\n            d_model=d_model,\n            num_heads=num_heads,\n            d_ff=d_ff,\n            num_layers=num_layers,\n            dropout=dropout,\n            max_len=max_len\n        )\n\n        # Decoder\n        self.decoder = Decoder(\n            vocab_size=trg_vocab_size,\n            d_model=d_model,\n            num_heads=num_heads,\n            d_ff=d_ff,\n            num_layers=num_layers,\n            dropout=dropout,\n            max_len=max_len\n        )\n\n        # Final projection to vocab size\n        self.output_layer = nn.Linear(d_model, trg_vocab_size)\n\n    def forward(self, src, trg, src_mask, trg_mask):\n        enc_out = self.encoder(src, src_mask)\n        dec_out, attn_weights = self.decoder(trg, enc_out, trg_mask, src_mask)\n        logits = self.output_layer(dec_out)\n        return logits, attn_weights\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T10:37:26.634458Z","iopub.execute_input":"2025-11-29T10:37:26.635180Z","iopub.status.idle":"2025-11-29T10:37:26.640671Z","shell.execute_reply.started":"2025-11-29T10:37:26.635154Z","shell.execute_reply":"2025-11-29T10:37:26.639889Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Masking ","metadata":{}},{"cell_type":"code","source":"def create_padding_mask(seq, pad_id=0):\n    mask = (seq != pad_id).unsqueeze(1).unsqueeze(2)  \n    # shape: (B, 1, 1, L)\n    return mask  # 1 for real tokens, 0 for pad\n\ndef create_subsequent_mask(size):\n    # Prevents attending to future tokens\n    mask = torch.tril(torch.ones(size, size)).bool()\n    # shape: (L, L)\n    return mask\n\ndef create_decoder_mask(trg, pad_id=0):\n    # padding mask\n    pad_mask = create_padding_mask(trg, pad_id)   # (B, 1, 1, L)\n\n    # future mask\n    seq_len = trg.size(1)\n    subseq_mask = create_subsequent_mask(seq_len).to(trg.device)  # (L, L)\n    subseq_mask = subseq_mask.unsqueeze(0).unsqueeze(1)           # (1, 1, L, L)\n\n    # Final mask: both padding + future\n    return pad_mask & subseq_mask\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T10:37:30.585028Z","iopub.execute_input":"2025-11-29T10:37:30.585285Z","iopub.status.idle":"2025-11-29T10:37:30.590424Z","shell.execute_reply.started":"2025-11-29T10:37:30.585265Z","shell.execute_reply":"2025-11-29T10:37:30.589720Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Loss Function","metadata":{}},{"cell_type":"code","source":"class LabelSmoothingLoss(nn.Module):\n    def __init__(self, label_smoothing, trg_vocab_size, ignore_index=0):\n        super().__init__()\n        self.src_vocab = trg_vocab_size\n        self.ignore_index = ignore_index\n        self.smoothing = label_smoothing\n        self.confidence = 1.0 - label_smoothing\n\n    def forward(self, pred, target):\n        # pred: (B, L, vocab)\n        # target: (B, L)\n\n        pred = pred.reshape(-1, pred.size(-1))     # FIXED\n        target = target.reshape(-1)                # FIXED\n\n        # ignore padding\n        mask = target != self.ignore_index\n        pred = pred[mask]\n        target = target[mask]\n\n        # log softmax\n        log_preds = F.log_softmax(pred, dim=-1)\n\n        # label smoothing\n        with torch.no_grad():\n            true_dist = torch.zeros_like(log_preds)\n            true_dist.fill_(self.smoothing / (self.src_vocab - 1))\n            true_dist.scatter_(1, target.unsqueeze(1), self.confidence)\n\n        loss = torch.mean(torch.sum(-true_dist * log_preds, dim=1))\n        return loss\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T10:37:32.313079Z","iopub.execute_input":"2025-11-29T10:37:32.313339Z","iopub.status.idle":"2025-11-29T10:37:32.319205Z","shell.execute_reply.started":"2025-11-29T10:37:32.313319Z","shell.execute_reply":"2025-11-29T10:37:32.318534Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nmodel = Transformer(\n    src_vocab_size=len(vocab_en),\n    trg_vocab_size=len(vocab_hi),\n    d_model=384,\n    num_heads=6,\n    num_layers=4,\n    d_ff=768,\n    dropout=0.1\n).to(device)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=3e-4)\n\nscheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.8)\n\n\nloss_fn = LabelSmoothingLoss(\n    label_smoothing=0.05,\n    trg_vocab_size=len(vocab_hi),\n    ignore_index=0\n)\n\nprint(\"Model initialized\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T10:38:32.819920Z","iopub.execute_input":"2025-11-29T10:38:32.820819Z","iopub.status.idle":"2025-11-29T10:38:32.978503Z","shell.execute_reply.started":"2025-11-29T10:38:32.820793Z","shell.execute_reply":"2025-11-29T10:38:32.977922Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Training loop","metadata":{}},{"cell_type":"code","source":"train_losses = []\nval_losses = []\n\nfor epoch in range(1, 6):   # changed from 3 to 6\n    train_loss = train_epoch(model, train_loader, optimizer, loss_fn)\n    val_loss = evaluate(model, val_loader, loss_fn)\n\n    scheduler.step()\n\n    train_losses.append(train_loss)\n    val_losses.append(val_loss)\n\n    print(f\"Epoch {epoch}: Train Loss = {train_loss:.4f}, Val Loss = {val_loss:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T10:38:34.345750Z","iopub.execute_input":"2025-11-29T10:38:34.346435Z","iopub.status.idle":"2025-11-29T10:39:21.071456Z","shell.execute_reply.started":"2025-11-29T10:38:34.346410Z","shell.execute_reply":"2025-11-29T10:39:21.070805Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Training loop via teacher forcing","metadata":{}},{"cell_type":"code","source":"print(\"Dataset size:\", len(df))\nprint(\"Train size:\", len(train_df))\nprint(\"Val size:\", len(val_df))\n\nprint(\"Number of batches in train_loader:\", len(train_loader))\nfor i, (src, trg) in enumerate(train_loader):\n    print(\"First batch shapes:\", src.shape, trg.shape)\n    break\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T10:41:43.855270Z","iopub.execute_input":"2025-11-29T10:41:43.855896Z","iopub.status.idle":"2025-11-29T10:41:43.862257Z","shell.execute_reply.started":"2025-11-29T10:41:43.855869Z","shell.execute_reply":"2025-11-29T10:41:43.861702Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train_epoch(model, train_loader, optimizer, loss_fn, pad_id=0, device=\"cuda\"):\n    model.train()\n    total_loss = 0\n\n    for src, trg in train_loader:\n        src, trg = src.to(device), trg.to(device)\n\n        # Create masks\n        src_mask = create_padding_mask(src, pad_id).to(device)\n        trg_mask = create_decoder_mask(trg, pad_id).to(device)\n\n        # Shift target for teacher forcing\n        trg_input = trg[:, :-1]\n        trg_output = trg[:, 1:]\n\n        # Forward pass\n        logits, _ = model(src, trg_input, src_mask, trg_mask[:, :, :-1, :-1])\n\n        loss = loss_fn(logits, trg_output)\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        total_loss += loss.item()\n\n    return total_loss / len(train_loader)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T10:41:45.018562Z","iopub.execute_input":"2025-11-29T10:41:45.019179Z","iopub.status.idle":"2025-11-29T10:41:45.024276Z","shell.execute_reply.started":"2025-11-29T10:41:45.019155Z","shell.execute_reply":"2025-11-29T10:41:45.023565Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Validation Loop","metadata":{}},{"cell_type":"code","source":"@torch.no_grad()\ndef evaluate(model, val_loader, loss_fn, pad_id=0, device=\"cuda\"):\n    model.eval()\n    total_loss = 0\n\n    for src, trg in val_loader:\n        src, trg = src.to(device), trg.to(device)\n\n        # masks\n        src_mask = create_padding_mask(src, pad_id).to(device)\n        trg_mask = create_decoder_mask(trg, pad_id).to(device)\n\n        trg_input = trg[:, :-1]\n        trg_output = trg[:, 1:]\n\n        logits, _ = model(src, trg_input, src_mask, trg_mask[:, :, :-1, :-1])\n        loss = loss_fn(logits, trg_output)\n\n        total_loss += loss.item()\n\n    return total_loss / len(val_loader)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T10:41:48.994353Z","iopub.execute_input":"2025-11-29T10:41:48.995077Z","iopub.status.idle":"2025-11-29T10:41:49.000055Z","shell.execute_reply.started":"2025-11-29T10:41:48.995050Z","shell.execute_reply":"2025-11-29T10:41:48.999344Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Greedy decoding and beam search","metadata":{}},{"cell_type":"markdown","source":"### Greedy decoding","metadata":{}},{"cell_type":"code","source":"@torch.no_grad()\ndef greedy_decode(model, src, src_mask, max_len, start_id=1, end_id=2, pad_id=0):\n    device = src.device\n\n    batch_size = src.size(0)\n\n    # Start with <sos>\n    trg_seq = torch.ones(batch_size, 1).fill_(start_id).long().to(device)\n\n    for _ in range(max_len):\n        trg_mask = create_decoder_mask(trg_seq, pad_id).to(device)\n\n        logits, _ = model(src, trg_seq, src_mask, trg_mask)\n        next_token = logits[:, -1, :].argmax(dim=-1).unsqueeze(1)\n\n        trg_seq = torch.cat([trg_seq, next_token], dim=1)\n\n        if (next_token == end_id).all():\n            break\n\n    return trg_seq\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T10:41:54.471010Z","iopub.execute_input":"2025-11-29T10:41:54.471272Z","iopub.status.idle":"2025-11-29T10:41:54.476187Z","shell.execute_reply.started":"2025-11-29T10:41:54.471252Z","shell.execute_reply":"2025-11-29T10:41:54.475596Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Beam search","metadata":{}},{"cell_type":"code","source":"import heapq\n\n@torch.no_grad()\ndef beam_search(model, src, src_mask, beam_width=3, max_len=40, start_id=1, end_id=2):\n    device = src.device\n\n    sequences = [[list([start_id]), 0.0]]  # (sequence, score)\n\n    for _ in range(max_len):\n        all_candidates = []\n\n        for seq, score in sequences:\n            trg_tensor = torch.tensor(seq).unsqueeze(0).to(device)\n            trg_mask = create_decoder_mask(trg_tensor).to(device)\n\n            logits, _ = model(src, trg_tensor, src_mask, trg_mask)\n            probs = F.log_softmax(logits[:, -1, :], dim=-1)\n\n            topk = torch.topk(probs, beam_width)\n            for i in range(beam_width):\n                token = topk.indices[0][i].item()\n                new_seq = seq + [token]\n                new_score = score + topk.values[0][i].item()\n                all_candidates.append([new_seq, new_score])\n\n        # Select best k sequences\n        ordered = sorted(all_candidates, key=lambda tup: tup[1], reverse=True)\n        sequences = ordered[:beam_width]\n\n        if all(seq[-1] == end_id for seq, _ in sequences):\n            break\n\n    best_seq = sequences[0][0]\n    return torch.tensor(best_seq)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T10:41:58.626894Z","iopub.execute_input":"2025-11-29T10:41:58.627167Z","iopub.status.idle":"2025-11-29T10:41:58.634051Z","shell.execute_reply.started":"2025-11-29T10:41:58.627145Z","shell.execute_reply":"2025-11-29T10:41:58.633273Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# BLEU and show sample translations","metadata":{}},{"cell_type":"code","source":"# Convert ids → tokens → sentence\ndef decode_ids(ids, vocab):\n    inv_vocab = {v: k for k, v in vocab.items()}\n    tokens = []\n    for i in ids:\n        if i in inv_vocab:\n            tok = inv_vocab[i]\n            if tok == \"<eos>\":\n                break\n            if tok not in [\"<pad>\", \"<sos>\"]:\n                tokens.append(tok)\n    return \" \".join(tokens)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T10:42:02.387281Z","iopub.execute_input":"2025-11-29T10:42:02.387539Z","iopub.status.idle":"2025-11-29T10:42:02.392427Z","shell.execute_reply.started":"2025-11-29T10:42:02.387519Z","shell.execute_reply":"2025-11-29T10:42:02.391732Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"MAX_EVAL = 500   # evaluate only 300 samples\n\ncount = 0\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T10:53:55.765330Z","iopub.execute_input":"2025-11-29T10:53:55.766077Z","iopub.status.idle":"2025-11-29T10:53:55.769394Z","shell.execute_reply.started":"2025-11-29T10:53:55.766050Z","shell.execute_reply":"2025-11-29T10:53:55.768666Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tqdm import tqdm\n\n@torch.no_grad()\ndef compute_bleu(model, val_loader, vocab_en, vocab_hi, pad_id=0, device=\"cuda\"):\n    model.eval()\n\n    references = []\n    hypotheses = []\n\n    # tqdm added here\n    for src, trg in tqdm(val_loader, desc=\"Computing BLEU\"):\n        src, trg = src.to(device), trg.to(device)\n\n        # process each sample separately\n        for b in range(src.size(0)):\n            src_b = src[b].unsqueeze(0)\n            trg_b = trg[b].unsqueeze(0)\n\n            src_mask = create_padding_mask(src_b, pad_id).to(device)\n\n            # Beam search with width=3\n            pred_ids = beam_search(model, src_b, src_mask, beam_width=3)\n            pred_ids = pred_ids.unsqueeze(0)\n\n            # decode predicted + target\n            pred = decode_ids(pred_ids[0].tolist(), vocab_hi)\n            tgt = decode_ids(trg_b[0].tolist(), vocab_hi)\n\n            hypotheses.append(pred)\n            references.append([tgt])\n\n    bleu = corpus_bleu(hypotheses, list(zip(*references)))\n    return bleu.score\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T10:56:28.016485Z","iopub.execute_input":"2025-11-29T10:56:28.017236Z","iopub.status.idle":"2025-11-29T10:56:28.023250Z","shell.execute_reply.started":"2025-11-29T10:56:28.017211Z","shell.execute_reply":"2025-11-29T10:56:28.022617Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"bleu_score = compute_bleu(model, val_loader, vocab_en, vocab_hi)\nprint(\"BLEU:\", bleu_score)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T10:56:32.267075Z","iopub.execute_input":"2025-11-29T10:56:32.267700Z","iopub.status.idle":"2025-11-29T11:07:18.369168Z","shell.execute_reply.started":"2025-11-29T10:56:32.267668Z","shell.execute_reply":"2025-11-29T11:07:18.368368Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Visualizations","metadata":{}},{"cell_type":"markdown","source":"## Train vs validation loss curves","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nplt.figure(figsize=(8,5))\nplt.plot(train_losses, label=\"Train Loss\")\nplt.plot(val_losses, label=\"Val Loss\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\nplt.title(\"Transformer Training Curve\")\nplt.legend()\nplt.grid()\nplt.savefig(\"loss_curve.png\", dpi=200)\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T11:37:14.748128Z","iopub.execute_input":"2025-11-29T11:37:14.748434Z","iopub.status.idle":"2025-11-29T11:37:15.201065Z","shell.execute_reply.started":"2025-11-29T11:37:14.748414Z","shell.execute_reply":"2025-11-29T11:37:15.200473Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# sample translations for github","metadata":{}},{"cell_type":"code","source":"samples = []\n\nfor i in range(5):   # take 5 examples\n    # Convert list → tensor\n    src_ids = torch.tensor(val_loader.dataset[i][0])\n    trg_ids = torch.tensor(val_loader.dataset[i][1])\n\n    # Decode English\n    eng = decode_ids(src_ids.tolist(), vocab_en)\n\n    # Predict Hindi\n    hin_pred = translate_sentence(model, eng)\n\n    # True Hindi\n    hin_true = decode_ids(trg_ids.tolist(), vocab_hi)\n\n    samples.append((eng, hin_pred, hin_true))\n\nimport pandas as pd\ndf_samples = pd.DataFrame(samples, columns=[\"English\", \"Predicted Hindi\", \"Actual Hindi\"])\ndf_samples.to_csv(\"sample_translations.csv\", index=False)\n\ndf_samples\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T11:38:54.540128Z","iopub.execute_input":"2025-11-29T11:38:54.540705Z","iopub.status.idle":"2025-11-29T11:39:01.820159Z","shell.execute_reply.started":"2025-11-29T11:38:54.540683Z","shell.execute_reply":"2025-11-29T11:39:01.819409Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Attention Heatmaps","metadata":{}},{"cell_type":"code","source":"@torch.no_grad()\ndef get_attention(model, src, trg):\n    model.eval()\n    src, trg = src.to(device), trg.to(device)\n    src_mask = create_padding_mask(src, 0).to(device)\n    trg_mask = create_decoder_mask(trg, 0).to(device)\n    logits, attn_weights = model(src, trg[:, :-1], src_mask, trg_mask[:, :, :-1, :-1])\n    return attn_weights[-1]  # last decoder layer\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T11:40:16.314434Z","iopub.execute_input":"2025-11-29T11:40:16.314767Z","iopub.status.idle":"2025-11-29T11:40:16.319378Z","shell.execute_reply.started":"2025-11-29T11:40:16.314744Z","shell.execute_reply":"2025-11-29T11:40:16.318836Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import seaborn as sns\n\ndef plot_cross_attention(attn, src_tokens, trg_tokens, head=0):\n    att = attn[0, head].cpu().numpy()\n    plt.figure(figsize=(10,6))\n    sns.heatmap(att, xticklabels=src_tokens, yticklabels=trg_tokens, cmap=\"viridis\")\n    plt.xlabel(\"Source Tokens\")\n    plt.ylabel(\"Target Tokens\")\n    plt.title(f\"Cross Attention Heatmap (Head {head})\")\n    plt.tight_layout()\n    plt.savefig(\"attention_heatmap.png\", dpi=200)\n    plt.show()\n\nsrc, trg = next(iter(val_loader))\nattn = get_attention(model, src, trg)\n\nsrc_tokens = decode_ids(src[0].tolist(), vocab_en).split()\ntrg_tokens = decode_ids(trg[0].tolist(), vocab_hi).split()\n\nplot_cross_attention(attn, src_tokens, trg_tokens)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T11:40:36.186723Z","iopub.execute_input":"2025-11-29T11:40:36.186992Z","iopub.status.idle":"2025-11-29T11:40:36.823040Z","shell.execute_reply.started":"2025-11-29T11:40:36.186972Z","shell.execute_reply":"2025-11-29T11:40:36.822409Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Saving everything","metadata":{}},{"cell_type":"code","source":"import json\n\nwith open(\"vocab_en.json\", \"w\") as f:\n    json.dump(vocab_en, f)\n\nwith open(\"vocab_hi.json\", \"w\") as f:\n    json.dump(vocab_hi, f)\ntorch.save(model.state_dict(), \"transformer_model.pth\")\n\nmetrics = {\n    \"train_losses\": train_losses,\n    \"val_losses\": val_losses,\n    \"bleu_score\": bleu_score\n}\npd.DataFrame(metrics).to_csv(\"metrics.csv\", index=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T11:41:25.995414Z","iopub.execute_input":"2025-11-29T11:41:25.996268Z","iopub.status.idle":"2025-11-29T11:41:26.103235Z","shell.execute_reply.started":"2025-11-29T11:41:25.996231Z","shell.execute_reply":"2025-11-29T11:41:26.102676Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import zipfile\nimport os\n\nfiles_to_zip = [\n    \"transformer_model.pth\",\n    \"vocab_en.json\",\n    \"vocab_hi.json\",\n    \"metrics.csv\",\n    \"loss_curve.png\",\n    \"attention_heatmap.png\",\n    \"sample_translations.csv\"\n]\n\nwith zipfile.ZipFile(\"transformer_project_outputs.zip\", \"w\") as zipf:\n    for f in files_to_zip:\n        if os.path.exists(f):\n            zipf.write(f)\n\nprint(\"ZIP created: transformer_project_outputs.zip\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T11:41:43.555889Z","iopub.execute_input":"2025-11-29T11:41:43.556152Z","iopub.status.idle":"2025-11-29T11:41:43.680825Z","shell.execute_reply.started":"2025-11-29T11:41:43.556133Z","shell.execute_reply":"2025-11-29T11:41:43.679961Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}