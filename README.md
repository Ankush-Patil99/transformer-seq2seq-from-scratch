<p align="center">
  <img src="https://img.shields.io/badge/PyTorch-EE4C2C?logo=pytorch&logoColor=white" alt="PyTorch"/>
  <img src="https://img.shields.io/badge/Python-3.10-blue?logo=python&logoColor=white" alt="Python"/>
  <img src="https://img.shields.io/badge/Model-Transformer-green" alt="Transformer"/>
  <img src="https://img.shields.io/badge/HuggingFace-Model-yellow?logo=huggingface&logoColor=white" alt="HF Model"/>
  <img src="https://img.shields.io/github/stars/Ankush-Patil99/transformer-seq2seq-from-scratch?style=social" alt="GitHub Stars"/>
</p>

# ğŸš€ Transformer English â†’ Hindi Translation (From Scratch in PyTorch)

This repository contains a complete **Transformer Encoderâ€“Decoder architecture implemented entirely from scratch** using PyTorch.  
Inspired by **â€œAttention Is All You Needâ€ (Vaswani et al., 2017)**, the project manually implements every component of the Transformer without using `torch.nn.Transformer`.

It is designed for **education, research, and professional ML/NLP portfolio demonstration**.
<details>
<summary><h2>ğŸ“š Table of Contents</h2></summary>

- [ğŸŒ Pretrained Model (HuggingFace Hub)](#-pretrained-model-huggingface-hub)
- [ğŸ“˜ Project Links](#-project-links-github-navigation)
- [ğŸ“š Dataset](#-dataset)
- [ğŸ§  Model Architecture](#-model-architecture)
- [âš™ï¸ Installation](#ï¸-installation)
- [ğŸ‹ï¸ Training Instructions](#ï¸-training-instructions)
- [ğŸ§ª Inference After Loading Model](#-inference-after-loading-model)
- [ğŸ¯ Compute BLEU Score](#-compute-bleu-score)
- [ğŸ”¥ Visualizations](#-visualizations)
- [ğŸ“Š Results Summary](#-results-summary)
- [ğŸ“ Sample Translations](#-sample-translations)
- [ğŸ”¥ Why This Project Matters](#-why-this-project-matters)
- [ğŸ”® Future Work Suggestions](#-future-work-suggestions)
- [ğŸ“„ License](#-license)
- [ğŸ‘¨â€ğŸ’» Author](#-author)

</details>

---

# ğŸŒ Pretrained Model (HuggingFace Hub)

The trained model weights (`transformer_model.pth`) are hosted on Hugging Face:

ğŸ‘‰ **[Click here to download from HuggingFace](https://huggingface.co/ankpatil1203/transformer-eng-hin-from-scratch/blob/main/transformer_model.pth)**

### Load the model in Python:

```python
from huggingface_hub import hf_hub_download
import torch
from src.model import Transformer

# Download model from HuggingFace Hub
model_path = hf_hub_download(
    repo_id="ankpatil1203/transformer-eng-hin-from-scratch",
    filename="transformer_model.pth"
)

# Initialize architecture and load weights
model = Transformer(src_vocab_size, trg_vocab_size)
model.load_state_dict(torch.load(model_path, map_location="cpu"))
model.eval()

```

---

# ğŸ“˜ Project Links (GitHub Navigation)

| Component | Link |
|----------|------|
| ğŸ““ Jupyter Notebook | [Click here](https://github.com/Ankush-Patil99/transformer-seq2seq-from-scratch/blob/main/transformer-seq2seq-from-scratch/notebooks/transformers-eng-hin.ipynb) |
| ğŸ“Š Results (plots, metrics, translations) | [Click here](https://github.com/Ankush-Patil99/transformer-seq2seq-from-scratch/tree/main/transformer-seq2seq-from-scratch/results) |
| ğŸ”¤ Vocabulary Files | [Click here](https://github.com/Ankush-Patil99/transformer-seq2seq-from-scratch/tree/main/transformer-seq2seq-from-scratch/vocab) |
| ğŸ”— Dataset (Kaggle) | [Click here](https://www.kaggle.com/datasets/vaibhavkumar11/hindi-english-parallel-corpus) |

---

# ğŸ“š Dataset

This project uses the **Hindiâ€“English Parallel Corpus** from Kaggle.

Dataset link:  
ğŸ‘‰ **[Click here to view/download the dataset](https://www.kaggle.com/datasets/vaibhavkumar11/hindi-english-parallel-corpus)**

### âš  Important  
The dataset **is not included** in this repository due to size and license restrictions.

To train the model:

1. Download the dataset from Kaggle  
2. Place the file into:

```
data/raw/
```

3. `train.py` will automatically split into training & validation sets.

---

# ğŸ§  Model Architecture

The Transformer follows the Encoderâ€“Decoder architecture with:

- Multi-Head Self-Attention  
- Cross-Attention  
- Feed-Forward Networks  
- Positional Encoding  
- Layer Normalization  
- Skip Connections  
- Look-Ahead Masking  

### Diagram

```
Input â†’ Token Embedding â†’ Positional Encoding â†’ Encoder (N layers)
                                                        â†“
                       Target â†’ Embedding â†’ Positional Encoding â†’ Decoder (N layers)
                                                        â†“
                                            Linear â†’ Softmax â†’ Output Tokens
```

### Hyperparameters

| Component | Value |
|----------|--------|
| d_model | 256 |
| heads | 4 |
| encoder layers | 3 |
| decoder layers | 3 |
| FFN size | 512 |
| dropout | 0.1 |
| optimizer | Adam (lr = 3e-4) |
| loss | Label Smoothing Loss |
| decoding | Greedy & Beam Search |

---

# âš™ï¸ Installation

```bash
pip install torch einops sacrebleu matplotlib pandas huggingface_hub
```

---

# ğŸ‹ï¸ Training Instructions

Run training:

```bash
python src/train.py
```

The training script performs:

- Data loading  
- Cleaning & preprocessing  
- Tokenization  
- Vocabulary building  
- Train/validation split  
- Mask creation  
- Forward/backward passes  
- Label smoothing  
- BLEU evaluation  
- Loss tracking  
- Model saving  

---

## ğŸ§ª Inference After Loading Model

After calling `model.eval()`, you can translate English sentences to Hindi:

```python
import torch, json
from src.utils import preprocess_sentence, decode_ids, create_padding_mask
from src.model import beam_search

# Select device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)
model.eval()

# Load vocabularies
with open("vocab/vocab_en.json") as f:
    vocab_en = json.load(f)

with open("vocab/vocab_hi.json") as f:
    vocab_hi = json.load(f)

sentence = "give your application a workout"

# Convert input sentence â†’ token IDs
src_ids = preprocess_sentence(sentence, vocab_en)
src_tensor = torch.tensor([src_ids]).to(device)

# Build mask
src_mask = create_padding_mask(src_tensor, pad_id=0).to(device)

# Beam search decoding
pred_ids = beam_search(model, src_tensor, src_mask, beam_width=5)

# Convert token IDs â†’ Hindi text
translation = decode_ids(pred_ids.tolist(), vocab_hi)

print("Input      :", sentence)
print("Translation:", translation)

```

---

# ğŸ¯ Compute BLEU Score

```python
from src.utils import compute_bleu

bleu = compute_bleu(model, val_loader, vocab_en, vocab_hi)
print("BLEU Score:", bleu)
```

---

# ğŸ”¥ Visualizations

### ğŸ“‰ Training Loss Curve  
A plot showing how the training loss decreases over epochs.  
ğŸ‘‰ **[Click here to view](https://github.com/Ankush-Patil99/transformer-seq2seq-from-scratch/blob/main/transformer-seq2seq-from-scratch/results/images/loss_curve.png)**

### ğŸ¯ Attention Heatmap  
This heatmap visualizes how the model attends to English tokens while generating Hindi output.  
ğŸ‘‰ **[Click here to view](https://github.com/Ankush-Patil99/transformer-seq2seq-from-scratch/blob/main/transformer-seq2seq-from-scratch/results/images/attention_heatmap.png)**


---

# ğŸ“Š Results Summary

| Metric | Value |
|--------|--------|
| BLEU Score | **49.76** |
| Final Training Loss | **0.8258** |
| Final Validation Loss | **1.0011** |
| Decoding Method | Beam Search (beam = 5) |

---

# ğŸ“ Sample Translations

A file containing sample model predictions (English â†’ Hindi translations) is available here:

ğŸ‘‰ **[Click here to view sample_translations.csv](https://github.com/Ankush-Patil99/transformer-seq2seq-from-scratch/blob/main/transformer-seq2seq-from-scratch/results/metrics/sample_translations.csv)**

---

# ğŸ”¥ Why This Project Matters

This project demonstrates:

- Strong understanding of Transformer internals  
- Ability to implement NLP architectures manually  
- Clean engineering practices  
- Multi-step training & evaluation workflow  
- Visualization and analysis skills  
- Integration with HuggingFace Hub  
- Portfolio-ready structure and documentation  

Perfect for ML Engineer / NLP Engineer roles.

---

# ğŸ”® Future Work Suggestions

- Add LSTM / GRU Seq2Seq baseline for comparison  
- Add Gradio UI for real-time translation  
- Train a deeper Transformer  
- Add FastAPI-based deployment  
- Use pretrained embeddings (FastText, GloVe)  
- Add mixed-precision training  

---

# ğŸ“„ License

MIT License

---

# ğŸ‘¨â€ğŸ’» Author

**Ankush Patil**  
Machine Learning & NLP Engineer  
Deep Learning | Transformers | PyTorch

- ğŸ“§ Email: **ankushpatil1203@gmail.com**
- ğŸ™ GitHub: [github.com/Ankush-Patil99](https://github.com/Ankush-Patil99)
- ğŸ’¼ LinkedIn: [linkedin.com/in/ankush-patil-91603a233](https://www.linkedin.com/in/ankush-patil-91603a233/)

---

# â­ Support

If you found this project helpful, please consider giving it a **GitHub Star** â­
